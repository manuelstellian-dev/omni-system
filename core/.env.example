# OMNI System - Environment Variables Configuration
# Copy this file to .env and fill in your values

# ============================================
# LLM Configuration
# ============================================

# Model to use for code generation
# Supported: gemini/gemini-2.5-flash, gpt-4o, claude-3-5-sonnet
OMNI_MODEL=gemini/gemini-2.5-flash

# API Keys (choose one based on your model)
GEMINI_API_KEY=your_gemini_api_key_here
OPENAI_API_KEY=your_openai_api_key_here
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# ============================================
# Concurrency & Performance
# ============================================

# Maximum concurrent tasks in SwarmAgent
# Options: 
#   - "auto" (default): Calculate based on available RAM
#   - Integer (1-10): Force specific number of concurrent tasks
# Recommended: auto (safe), or 3-5 for systems with 8GB+ RAM
OMNI_MAX_CONCURRENT_TASKS=auto

# Memory threshold (GB) before reducing concurrency
# When available RAM drops below this, system reduces parallel execution
OMNI_MEMORY_THRESHOLD_GB=2

# ============================================
# ChromaDB Configuration
# ============================================

# Path to ChromaDB persistent storage
OMNI_MEMORY_PATH=.omni_memory

# Number of results to retrieve for RAG context
OMNI_MEMORY_CONTEXT_SIZE=5

# ============================================
# Build & Verification
# ============================================

# Timeout for build verification (seconds)
OMNI_BUILD_TIMEOUT=600

# Maximum repair attempts before giving up
OMNI_MAX_REPAIR_ATTEMPTS=7

# ============================================
# Logging & Debugging
# ============================================

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
OMNI_LOG_LEVEL=INFO

# Enable verbose output
OMNI_VERBOSE=false

# Enable debug mode (detailed error traces)
OMNI_DEBUG=false

# ============================================
# Advanced Configuration
# ============================================

# LLM temperature for code generation (0.0-1.0)
# Lower = more deterministic, Higher = more creative
OMNI_TEMPERATURE=0.2

# Maximum tokens for LLM responses
OMNI_MAX_TOKENS=4096

# Retry attempts for failed LLM calls
OMNI_LLM_RETRY_ATTEMPTS=3

# ============================================
# Optional: Deployment Configuration
# ============================================

# Railway API Token (for automatic deployment)
# RAILWAY_TOKEN=your_railway_token_here

# Vercel Token (for Next.js deployments)
# VERCEL_TOKEN=your_vercel_token_here

# GitHub Token (for CI/CD and releases)
# GITHUB_TOKEN=your_github_token_here

# ============================================
# Notes
# ============================================

# - Never commit the .env file to Git!
# - Keep your API keys secure
# - Use environment-specific configurations for different stages
# - For production, use secret management tools (AWS Secrets Manager, etc.)
